{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba947328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "import random\n",
    "from tensorflow.keras import layers, Input\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1ad27b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_to_array(nb):\n",
    "    list = []\n",
    "    for i in range(nb):\n",
    "        list.append(0)\n",
    "    list.append(1)\n",
    "    for i in range(10-nb-1):\n",
    "        list.append(0)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "280c48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6faa4b8",
   "metadata": {},
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_train = tf.image.resize(X_train, [7, 7])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0eb2489b",
   "metadata": {},
   "source": [
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "X_test = tf.image.resize(X_test, [7, 7])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aedb23f9",
   "metadata": {},
   "source": [
    "X_train = X_train.numpy()\n",
    "X_test = X_test.numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "547c69b1",
   "metadata": {},
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1bf92552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "73c978e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train >= 128] = 255\n",
    "X_test[X_test >= 128] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "915d48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train < 128] = 0\n",
    "X_test[X_test < 128] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c85267a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d1b8e285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fe4d7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_m = np.array([nb_to_array(5)])\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_train_m = np.append(y_train_m, [nb_to_array(y_train[i])], axis=0)\n",
    "y_train_m = np.delete(y_train_m, 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9aa50c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_m = np.array([nb_to_array(5)])\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_m = np.append(y_test_m, [nb_to_array(y_test[i])], axis=0)\n",
    "y_test_m = np.delete(y_test_m, 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "811ba80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_m[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0298262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_m[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c618a03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5fe87cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant normalisation : Minimum = 0, Maximum = 255\n",
      "Après normalisation : Minimum = 0.0, Maximum = 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Avant normalisation : Minimum = '+str(X_train.min())+', Maximum = '+str(X_train.max()))\n",
    "\n",
    "X_max = X_train.max()\n",
    "X_train  = X_train / X_max\n",
    "X_test = X_test / X_max\n",
    "\n",
    "print('Après normalisation : Minimum = '+str(X_train.min())+', Maximum = '+str(X_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a6f3c28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3fe6662e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger\n",
    "    plt.imshow(X_test[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_test[i]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d875446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b9dfea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5e08dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ab2f8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7e44adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000, 1)\n",
      "(10000, 1)\n",
      "=============\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(\"=============\")\n",
    "print(y_train_m.shape)\n",
    "print(y_test_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "da366cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Input(shape=(784,)))\n",
    "\n",
    "model.add(layers.Dense(256, activation=\"relu\")),\n",
    "\n",
    "model.add(layers.Dense(128, activation=\"relu\")),\n",
    "\n",
    "model.add(layers.Dense(64, activation=\"relu\")),\n",
    "\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c9553aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"mae\",\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e3cbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=20, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint('model-mnist.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1237f072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1850/1875 [============================>.] - ETA: 0s - loss: 0.7329 - mean_absolute_error: 0.7329\n",
      "Epoch 00001: val_loss improved from inf to 0.45873, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.7297 - mean_absolute_error: 0.7297 - val_loss: 0.4587 - val_mean_absolute_error: 0.4587\n",
      "Epoch 2/10000\n",
      "1856/1875 [============================>.] - ETA: 0s - loss: 0.3530 - mean_absolute_error: 0.3530\n",
      "Epoch 00002: val_loss improved from 0.45873 to 0.28331, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 961us/step - loss: 0.3525 - mean_absolute_error: 0.3525 - val_loss: 0.2833 - val_mean_absolute_error: 0.2833\n",
      "Epoch 3/10000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.2281 - mean_absolute_error: 0.2281\n",
      "Epoch 00003: val_loss improved from 0.28331 to 0.23240, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 992us/step - loss: 0.2278 - mean_absolute_error: 0.2278 - val_loss: 0.2324 - val_mean_absolute_error: 0.2324\n",
      "Epoch 4/10000\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.1761 - mean_absolute_error: 0.1761\n",
      "Epoch 00004: val_loss improved from 0.23240 to 0.18825, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1757 - mean_absolute_error: 0.1757 - val_loss: 0.1883 - val_mean_absolute_error: 0.1883\n",
      "Epoch 5/10000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.1446 - mean_absolute_error: 0.1446\n",
      "Epoch 00005: val_loss improved from 0.18825 to 0.18504, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1446 - mean_absolute_error: 0.1446 - val_loss: 0.1850 - val_mean_absolute_error: 0.1850\n",
      "Epoch 6/10000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.1247 - mean_absolute_error: 0.1247\n",
      "Epoch 00006: val_loss improved from 0.18504 to 0.17148, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1247 - mean_absolute_error: 0.1247 - val_loss: 0.1715 - val_mean_absolute_error: 0.1715\n",
      "Epoch 7/10000\n",
      "1843/1875 [============================>.] - ETA: 0s - loss: 0.1115 - mean_absolute_error: 0.1115\n",
      "Epoch 00007: val_loss improved from 0.17148 to 0.16264, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1113 - mean_absolute_error: 0.1113 - val_loss: 0.1626 - val_mean_absolute_error: 0.1626\n",
      "Epoch 8/10000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0991 - mean_absolute_error: 0.0991\n",
      "Epoch 00008: val_loss improved from 0.16264 to 0.16139, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0991 - mean_absolute_error: 0.0991 - val_loss: 0.1614 - val_mean_absolute_error: 0.1614\n",
      "Epoch 9/10000\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0935 - mean_absolute_error: 0.0935\n",
      "Epoch 00009: val_loss did not improve from 0.16139\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0935 - mean_absolute_error: 0.0935 - val_loss: 0.1754 - val_mean_absolute_error: 0.1754\n",
      "Epoch 10/10000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0838 - mean_absolute_error: 0.0838\n",
      "Epoch 00010: val_loss improved from 0.16139 to 0.15559, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0839 - mean_absolute_error: 0.0839 - val_loss: 0.1556 - val_mean_absolute_error: 0.1556\n",
      "Epoch 11/10000\n",
      "1857/1875 [============================>.] - ETA: 0s - loss: 0.0790 - mean_absolute_error: 0.0790\n",
      "Epoch 00011: val_loss improved from 0.15559 to 0.14673, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0791 - mean_absolute_error: 0.0791 - val_loss: 0.1467 - val_mean_absolute_error: 0.1467\n",
      "Epoch 12/10000\n",
      "1832/1875 [============================>.] - ETA: 0s - loss: 0.0749 - mean_absolute_error: 0.0749\n",
      "Epoch 00012: val_loss improved from 0.14673 to 0.14167, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0750 - mean_absolute_error: 0.0750 - val_loss: 0.1417 - val_mean_absolute_error: 0.1417\n",
      "Epoch 13/10000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0718 - mean_absolute_error: 0.0718\n",
      "Epoch 00013: val_loss did not improve from 0.14167\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0719 - mean_absolute_error: 0.0719 - val_loss: 0.1487 - val_mean_absolute_error: 0.1487\n",
      "Epoch 14/10000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0657 - mean_absolute_error: 0.0657\n",
      "Epoch 00014: val_loss did not improve from 0.14167\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0657 - mean_absolute_error: 0.0657 - val_loss: 0.1751 - val_mean_absolute_error: 0.1751\n",
      "Epoch 15/10000\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 0.0631 - mean_absolute_error: 0.0631\n",
      "Epoch 00015: val_loss did not improve from 0.14167\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0633 - mean_absolute_error: 0.0633 - val_loss: 0.1475 - val_mean_absolute_error: 0.1475\n",
      "Epoch 16/10000\n",
      "1830/1875 [============================>.] - ETA: 0s - loss: 0.0605 - mean_absolute_error: 0.0605\n",
      "Epoch 00016: val_loss improved from 0.14167 to 0.13687, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0607 - mean_absolute_error: 0.0607 - val_loss: 0.1369 - val_mean_absolute_error: 0.1369\n",
      "Epoch 17/10000\n",
      "1851/1875 [============================>.] - ETA: 0s - loss: 0.0560 - mean_absolute_error: 0.0560\n",
      "Epoch 00017: val_loss did not improve from 0.13687\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0560 - mean_absolute_error: 0.0560 - val_loss: 0.1534 - val_mean_absolute_error: 0.1534\n",
      "Epoch 18/10000\n",
      "1856/1875 [============================>.] - ETA: 0s - loss: 0.0553 - mean_absolute_error: 0.0553\n",
      "Epoch 00018: val_loss improved from 0.13687 to 0.13536, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0553 - mean_absolute_error: 0.0553 - val_loss: 0.1354 - val_mean_absolute_error: 0.1354\n",
      "Epoch 19/10000\n",
      "1830/1875 [============================>.] - ETA: 0s - loss: 0.0533 - mean_absolute_error: 0.0533\n",
      "Epoch 00019: val_loss improved from 0.13536 to 0.13478, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0534 - mean_absolute_error: 0.0534 - val_loss: 0.1348 - val_mean_absolute_error: 0.1348\n",
      "Epoch 20/10000\n",
      "1837/1875 [============================>.] - ETA: 0s - loss: 0.0520 - mean_absolute_error: 0.0520\n",
      "Epoch 00020: val_loss improved from 0.13478 to 0.12882, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0519 - mean_absolute_error: 0.0519 - val_loss: 0.1288 - val_mean_absolute_error: 0.1288\n",
      "Epoch 21/10000\n",
      "1821/1875 [============================>.] - ETA: 0s - loss: 0.0486 - mean_absolute_error: 0.0486\n",
      "Epoch 00021: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0486 - mean_absolute_error: 0.0486 - val_loss: 0.1322 - val_mean_absolute_error: 0.1322\n",
      "Epoch 22/10000\n",
      "1844/1875 [============================>.] - ETA: 0s - loss: 0.0462 - mean_absolute_error: 0.0462\n",
      "Epoch 00022: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0462 - mean_absolute_error: 0.0462 - val_loss: 0.1487 - val_mean_absolute_error: 0.1487\n",
      "Epoch 23/10000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0446 - mean_absolute_error: 0.0446\n",
      "Epoch 00023: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0448 - mean_absolute_error: 0.0448 - val_loss: 0.1359 - val_mean_absolute_error: 0.1359\n",
      "Epoch 24/10000\n",
      "1838/1875 [============================>.] - ETA: 0s - loss: 0.0453 - mean_absolute_error: 0.0453\n",
      "Epoch 00024: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0452 - mean_absolute_error: 0.0452 - val_loss: 0.1364 - val_mean_absolute_error: 0.1364\n",
      "Epoch 25/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1833/1875 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.0440\n",
      "Epoch 00025: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0438 - mean_absolute_error: 0.0438 - val_loss: 0.1383 - val_mean_absolute_error: 0.1383\n",
      "Epoch 26/10000\n",
      "1853/1875 [============================>.] - ETA: 0s - loss: 0.0449 - mean_absolute_error: 0.0449\n",
      "Epoch 00026: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0449 - mean_absolute_error: 0.0449 - val_loss: 0.1367 - val_mean_absolute_error: 0.1367\n",
      "Epoch 27/10000\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0417 - mean_absolute_error: 0.0417\n",
      "Epoch 00027: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0417 - mean_absolute_error: 0.0417 - val_loss: 0.1310 - val_mean_absolute_error: 0.1310\n",
      "Epoch 28/10000\n",
      "1827/1875 [============================>.] - ETA: 0s - loss: 0.0418 - mean_absolute_error: 0.0418\n",
      "Epoch 00028: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0418 - mean_absolute_error: 0.0418 - val_loss: 0.1384 - val_mean_absolute_error: 0.1384\n",
      "Epoch 29/10000\n",
      "1837/1875 [============================>.] - ETA: 0s - loss: 0.0407 - mean_absolute_error: 0.0407\n",
      "Epoch 00029: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0409 - mean_absolute_error: 0.0409 - val_loss: 0.1348 - val_mean_absolute_error: 0.1348\n",
      "Epoch 30/10000\n",
      "1842/1875 [============================>.] - ETA: 0s - loss: 0.0396 - mean_absolute_error: 0.0396\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12882\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0397 - mean_absolute_error: 0.0397 - val_loss: 0.1333 - val_mean_absolute_error: 0.1333\n",
      "Epoch 31/10000\n",
      "1843/1875 [============================>.] - ETA: 0s - loss: 0.0215 - mean_absolute_error: 0.0215\n",
      "Epoch 00031: val_loss improved from 0.12882 to 0.12495, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0215 - mean_absolute_error: 0.0215 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "Epoch 32/10000\n",
      "1845/1875 [============================>.] - ETA: 0s - loss: 0.0185 - mean_absolute_error: 0.0185\n",
      "Epoch 00032: val_loss improved from 0.12495 to 0.12084, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0184 - mean_absolute_error: 0.0184 - val_loss: 0.1208 - val_mean_absolute_error: 0.1208\n",
      "Epoch 33/10000\n",
      "1827/1875 [============================>.] - ETA: 0s - loss: 0.0169 - mean_absolute_error: 0.0169\n",
      "Epoch 00033: val_loss did not improve from 0.12084\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0172 - mean_absolute_error: 0.0172 - val_loss: 0.1214 - val_mean_absolute_error: 0.1214\n",
      "Epoch 34/10000\n",
      "1843/1875 [============================>.] - ETA: 0s - loss: 0.0166 - mean_absolute_error: 0.0166\n",
      "Epoch 00034: val_loss did not improve from 0.12084\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0166 - mean_absolute_error: 0.0166 - val_loss: 0.1221 - val_mean_absolute_error: 0.1221\n",
      "Epoch 35/10000\n",
      "1862/1875 [============================>.] - ETA: 0s - loss: 0.0160 - mean_absolute_error: 0.0160\n",
      "Epoch 00035: val_loss improved from 0.12084 to 0.11948, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0161 - mean_absolute_error: 0.0161 - val_loss: 0.1195 - val_mean_absolute_error: 0.1195\n",
      "Epoch 36/10000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0156 - mean_absolute_error: 0.0156\n",
      "Epoch 00036: val_loss improved from 0.11948 to 0.11790, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0157 - mean_absolute_error: 0.0157 - val_loss: 0.1179 - val_mean_absolute_error: 0.1179\n",
      "Epoch 37/10000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0155 - mean_absolute_error: 0.0155\n",
      "Epoch 00037: val_loss did not improve from 0.11790\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0155 - mean_absolute_error: 0.0155 - val_loss: 0.1186 - val_mean_absolute_error: 0.1186\n",
      "Epoch 38/10000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0154 - mean_absolute_error: 0.0154\n",
      "Epoch 00038: val_loss did not improve from 0.11790\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0154 - mean_absolute_error: 0.0154 - val_loss: 0.1194 - val_mean_absolute_error: 0.1194\n",
      "Epoch 39/10000\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0154 - mean_absolute_error: 0.0154\n",
      "Epoch 00039: val_loss did not improve from 0.11790\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0153 - mean_absolute_error: 0.0153 - val_loss: 0.1198 - val_mean_absolute_error: 0.1198\n",
      "Epoch 40/10000\n",
      "1852/1875 [============================>.] - ETA: 0s - loss: 0.0151 - mean_absolute_error: 0.0151\n",
      "Epoch 00040: val_loss did not improve from 0.11790\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0150 - mean_absolute_error: 0.0150 - val_loss: 0.1188 - val_mean_absolute_error: 0.1188\n",
      "Epoch 41/10000\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0150 - mean_absolute_error: 0.0150\n",
      "Epoch 00041: val_loss did not improve from 0.11790\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0150 - mean_absolute_error: 0.0150 - val_loss: 0.1191 - val_mean_absolute_error: 0.1191\n",
      "Epoch 42/10000\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.0150 - mean_absolute_error: 0.0150\n",
      "Epoch 00042: val_loss did not improve from 0.11790\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0149 - mean_absolute_error: 0.0149 - val_loss: 0.1186 - val_mean_absolute_error: 0.1186\n",
      "Epoch 43/10000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0148 - mean_absolute_error: 0.0148\n",
      "Epoch 00043: val_loss did not improve from 0.11790\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0149 - mean_absolute_error: 0.0149 - val_loss: 0.1196 - val_mean_absolute_error: 0.1196\n",
      "Epoch 44/10000\n",
      "1850/1875 [============================>.] - ETA: 0s - loss: 0.0145 - mean_absolute_error: 0.0145\n",
      "Epoch 00044: val_loss improved from 0.11790 to 0.11719, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0145 - mean_absolute_error: 0.0145 - val_loss: 0.1172 - val_mean_absolute_error: 0.1172\n",
      "Epoch 45/10000\n",
      "1850/1875 [============================>.] - ETA: 0s - loss: 0.0147 - mean_absolute_error: 0.0147\n",
      "Epoch 00045: val_loss did not improve from 0.11719\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0146 - mean_absolute_error: 0.0146 - val_loss: 0.1194 - val_mean_absolute_error: 0.1194\n",
      "Epoch 46/10000\n",
      "1849/1875 [============================>.] - ETA: 0s - loss: 0.0146 - mean_absolute_error: 0.0146\n",
      "Epoch 00046: val_loss did not improve from 0.11719\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0145 - mean_absolute_error: 0.0145 - val_loss: 0.1200 - val_mean_absolute_error: 0.1200\n",
      "Epoch 47/10000\n",
      "1844/1875 [============================>.] - ETA: 0s - loss: 0.0144 - mean_absolute_error: 0.0144\n",
      "Epoch 00047: val_loss did not improve from 0.11719\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.1193 - val_mean_absolute_error: 0.1193\n",
      "Epoch 48/10000\n",
      "1832/1875 [============================>.] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
      "Epoch 00048: val_loss improved from 0.11719 to 0.11705, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0143 - mean_absolute_error: 0.0143 - val_loss: 0.1171 - val_mean_absolute_error: 0.1171\n",
      "Epoch 49/10000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.0142\n",
      "Epoch 00049: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.1201 - val_mean_absolute_error: 0.1201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/10000\n",
      "1839/1875 [============================>.] - ETA: 0s - loss: 0.0144 - mean_absolute_error: 0.0144\n",
      "Epoch 00050: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.1184 - val_mean_absolute_error: 0.1184\n",
      "Epoch 51/10000\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.0141\n",
      "Epoch 00051: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0141 - mean_absolute_error: 0.0141 - val_loss: 0.1183 - val_mean_absolute_error: 0.1183\n",
      "Epoch 52/10000\n",
      "1830/1875 [============================>.] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.0141\n",
      "Epoch 00052: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0141 - mean_absolute_error: 0.0141 - val_loss: 0.1175 - val_mean_absolute_error: 0.1175\n",
      "Epoch 53/10000\n",
      "1852/1875 [============================>.] - ETA: 0s - loss: 0.0139 - mean_absolute_error: 0.0139\n",
      "Epoch 00053: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0140 - mean_absolute_error: 0.0140 - val_loss: 0.1186 - val_mean_absolute_error: 0.1186\n",
      "Epoch 54/10000\n",
      "1853/1875 [============================>.] - ETA: 0s - loss: 0.0140 - mean_absolute_error: 0.0140\n",
      "Epoch 00054: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0140 - mean_absolute_error: 0.0140 - val_loss: 0.1189 - val_mean_absolute_error: 0.1189\n",
      "Epoch 55/10000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0139 - mean_absolute_error: 0.0139\n",
      "Epoch 00055: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0140 - mean_absolute_error: 0.0140 - val_loss: 0.1205 - val_mean_absolute_error: 0.1205\n",
      "Epoch 56/10000\n",
      "1831/1875 [============================>.] - ETA: 0s - loss: 0.0138 - mean_absolute_error: 0.0138\n",
      "Epoch 00056: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0140 - mean_absolute_error: 0.0140 - val_loss: 0.1185 - val_mean_absolute_error: 0.1185\n",
      "Epoch 57/10000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0140 - mean_absolute_error: 0.0140\n",
      "Epoch 00057: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0140 - mean_absolute_error: 0.0140 - val_loss: 0.1203 - val_mean_absolute_error: 0.1203\n",
      "Epoch 58/10000\n",
      "1851/1875 [============================>.] - ETA: 0s - loss: 0.0137 - mean_absolute_error: 0.0137\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0137 - mean_absolute_error: 0.0137 - val_loss: 0.1194 - val_mean_absolute_error: 0.1194\n",
      "Epoch 59/10000\n",
      "1850/1875 [============================>.] - ETA: 0s - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 00059: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0127 - mean_absolute_error: 0.0127 - val_loss: 0.1177 - val_mean_absolute_error: 0.1177\n",
      "Epoch 60/10000\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0126 - mean_absolute_error: 0.0126\n",
      "Epoch 00060: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0125 - mean_absolute_error: 0.0125 - val_loss: 0.1173 - val_mean_absolute_error: 0.1173\n",
      "Epoch 61/10000\n",
      "1834/1875 [============================>.] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124- ETA: 1s - loss: 0.01\n",
      "Epoch 00061: val_loss did not improve from 0.11705\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.1171 - val_mean_absolute_error: 0.1171\n",
      "Epoch 62/10000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0123 - mean_absolute_error: 0.0123\n",
      "Epoch 00062: val_loss improved from 0.11705 to 0.11693, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0123 - mean_absolute_error: 0.0123 - val_loss: 0.1169 - val_mean_absolute_error: 0.1169\n",
      "Epoch 63/10000\n",
      "1832/1875 [============================>.] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 00063: val_loss did not improve from 0.11693\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.1172 - val_mean_absolute_error: 0.1172\n",
      "Epoch 64/10000\n",
      "1845/1875 [============================>.] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 00064: val_loss improved from 0.11693 to 0.11687, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0122 - mean_absolute_error: 0.0122 - val_loss: 0.1169 - val_mean_absolute_error: 0.1169\n",
      "Epoch 65/10000\n",
      "1835/1875 [============================>.] - ETA: 0s - loss: 0.0122 - mean_absolute_error: 0.0122\n",
      "Epoch 00065: val_loss did not improve from 0.11687\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.1172 - val_mean_absolute_error: 0.1172\n",
      "Epoch 66/10000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 00066: val_loss improved from 0.11687 to 0.11685, saving model to model-mnist.h5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.1169 - val_mean_absolute_error: 0.1169\n",
      "Epoch 67/10000\n",
      "1857/1875 [============================>.] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 00067: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0121 - mean_absolute_error: 0.0121 - val_loss: 0.1170 - val_mean_absolute_error: 0.1170\n",
      "Epoch 68/10000\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00068: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.1170 - val_mean_absolute_error: 0.1170\n",
      "Epoch 69/10000\n",
      "1814/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00069: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.1173 - val_mean_absolute_error: 0.1173\n",
      "Epoch 70/10000\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00070: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.1174 - val_mean_absolute_error: 0.1174\n",
      "Epoch 71/10000\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00071: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.1173 - val_mean_absolute_error: 0.1173\n",
      "Epoch 72/10000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1177 - val_mean_absolute_error: 0.1177\n",
      "Epoch 73/10000\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 00073: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1176 - val_mean_absolute_error: 0.1176\n",
      "Epoch 74/10000\n",
      "1842/1875 [============================>.] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 00074: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1177 - val_mean_absolute_error: 0.1177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/10000\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 00075: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1173 - val_mean_absolute_error: 0.1173\n",
      "Epoch 76/10000\n",
      "1859/1875 [============================>.] - ETA: 0s - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 00076: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1176 - val_mean_absolute_error: 0.1176\n",
      "Epoch 77/10000\n",
      "1862/1875 [============================>.] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 00077: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1176 - val_mean_absolute_error: 0.1176\n",
      "Epoch 78/10000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00078: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1180 - val_mean_absolute_error: 0.1180\n",
      "Epoch 79/10000\n",
      "1851/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00079: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1177 - val_mean_absolute_error: 0.1177\n",
      "Epoch 80/10000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00080: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1179 - val_mean_absolute_error: 0.1179\n",
      "Epoch 81/10000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00081: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1177 - val_mean_absolute_error: 0.1177\n",
      "Epoch 82/10000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00082: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1179 - val_mean_absolute_error: 0.1179\n",
      "Epoch 83/10000\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 00083: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1178 - val_mean_absolute_error: 0.1178\n",
      "Epoch 84/10000\n",
      "1847/1875 [============================>.] - ETA: 0s - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 00084: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1178 - val_mean_absolute_error: 0.1178\n",
      "Epoch 85/10000\n",
      "1824/1875 [============================>.] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 00085: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1180 - val_mean_absolute_error: 0.1180\n",
      "Epoch 86/10000\n",
      "1830/1875 [============================>.] - ETA: 0s - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 00086: val_loss did not improve from 0.11685\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0119 - mean_absolute_error: 0.0119 - val_loss: 0.1180 - val_mean_absolute_error: 0.1180\n",
      "Epoch 00086: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train, \n",
    "    y=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10000,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=my_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fe554d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE for Model: 0.11800917983055115\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "val_mae = mean_absolute_error(y_pred, y_test)\n",
    "print(\"Validation MAE for Model: {}\".format(val_mae))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0216303a",
   "metadata": {},
   "source": [
    "model = keras.models.load_model('model-mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1b7c74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e4ee7c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "57ef7e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "852f1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_pred.shape[0]):\n",
    "    y_pred[i, 0] = round(y_pred[i, 0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4bd9734",
   "metadata": {},
   "source": [
    "y_pred = y_pred.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "be935873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [4.],\n",
       "       [1.],\n",
       "       [4.],\n",
       "       [9.],\n",
       "       [6.],\n",
       "       [9.]], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "20d06b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [4],\n",
       "       [1],\n",
       "       [4],\n",
       "       [9],\n",
       "       [5],\n",
       "       [9]], dtype=uint8)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aac7555c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pourcentage erreur = 4.3 , 430/10000\n"
     ]
    }
   ],
   "source": [
    "nb_erreur = 0\n",
    "tab = []\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        nb_erreur += 1\n",
    "        tab.append(i)\n",
    "print(\"pourcentage erreur = \"+str(nb_erreur/y_pred.shape[0]*100)+\" , \"+str(nb_erreur)+\"/\"+str(y_pred.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f415efe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (49,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [112]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# Make the figures a bit bigger\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_test[tab[i]]))\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu-cuda8\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    454\u001b[0m     warn_deprecated(\n\u001b[0;32m    455\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    458\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu-cuda8\\lib\\site-packages\\matplotlib\\pyplot.py:2652\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2646\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   2647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   2648\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2649\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2650\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[0;32m   2651\u001b[0m         resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2652\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2654\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2656\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2659\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2660\u001b[0m     sci(__ret)\n\u001b[0;32m   2661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu-cuda8\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    454\u001b[0m     warn_deprecated(\n\u001b[0;32m    455\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    458\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu-cuda8\\lib\\site-packages\\matplotlib\\__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu-cuda8\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5481\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5475\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap, norm, interpolation,\n\u001b[0;32m   5476\u001b[0m                       origin, extent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   5477\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   5478\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5479\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 5481\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5482\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5484\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu-cuda8\\lib\\site-packages\\matplotlib\\image.py:715\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    716\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    720\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (49,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger\n",
    "for i in range(len(tab)): \n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger\n",
    "    plt.imshow(X_test[tab[i]], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_test[tab[i]]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "54572cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ef4330c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "axe = plt.axes()\n",
    "plt.rcParams.update({'font.size':15})\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.title('Euler ' + str(i+1))\n",
    "axe.set(xlabel=\"Valeurs réelles\", ylabel=\"Valeurs prédites\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1e363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
